{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NLP Over a SXSW Twitter Data Set**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package and Data Import"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Package Imports*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\moore\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\moore\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Packages to Import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import warnings\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Importing CSV File*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Data and Change Column Names\n",
    "\n",
    "df = pd.read_csv(\"data/tweets.csv\")\n",
    "df.columns = ['text', 'device', 'emotion']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dropping non-significant values and rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dropping 'I can't tell' and 'Other' rows\n",
    "\n",
    "df = df[df['emotion'] != \"I can't tell\"]\n",
    "\n",
    "### Dropping blank 'text' rows\n",
    "\n",
    "df = df.dropna(subset=['text'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating functions that clean the text data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a function that removes words that begin with @, as mentions would not be important in determining the emotion of a tweet\n",
    "\n",
    "def remove_at(text):\n",
    "    text = text.split()\n",
    "    text = [word for word in text if not word.startswith('@')]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "### Creating a function that makes all text lowercase for further analysis\n",
    "\n",
    "def lower_case(text):\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "### Creating a function that removes all punctuation with the exception of ! and ? as they may be important in determining the emotion of a tweet\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    text = text.split()\n",
    "    text = [word for word in text if not word.startswith('!') and not word.startswith('?')]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "### Creating a function that removes stopwords from a specified list of stopwords\n",
    "\n",
    "custom_stop_words = ['in','of','at','a','the']\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = text.split()\n",
    "    text = [word for word in text if word not in custom_stop_words]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "### Creating a function that removes non-ASCII characters\n",
    "\n",
    "def remove_characters(text):\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    return text\n",
    "\n",
    "### Creating a function that lemmatizes words\n",
    "\n",
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = text.split()\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "### Creating a function takes the tokenized text and returns a string of words\n",
    "\n",
    "def tokenize_to_string(text):\n",
    "    tknzr = TweetTokenizer()\n",
    "    text = tknzr.tokenize(text)\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "### Creating a function that removes '#SXSW' of any case type from the text\n",
    "\n",
    "def remove_sxsw(text):\n",
    "    text = text.split()\n",
    "    text = [word for word in text if not word.startswith('#sxsw') and not word.startswith('#SXSW')]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "### Creating a function that combines all of the above functions\n",
    "\n",
    "def clean_text(text):\n",
    "    text = remove_at(text)\n",
    "    text = lower_case(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = remove_characters(text)\n",
    "    text = lemmatize(text)\n",
    "    text = tokenize_to_string(text)\n",
    "    text = remove_sxsw(text)\n",
    "    return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating 'device_type'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Deliniating between Google and Apple\n",
    "\n",
    "google_tweets = ['Google', 'Other Google product or service', 'Andriod App', 'Andriod']\n",
    "apple_tweets = ['Apple', 'Other Apple product or service', 'Apple App', 'iPhone', 'iPad', 'iPad or iPhone App']\n",
    "\n",
    "### Creating a new column for google vs. apple vs. unknown\n",
    "\n",
    "df['device_type'] = np.where(df['device'].isin(google_tweets), 'Google', \n",
    "                    np.where(df['device'].isin(apple_tweets), 'Apple', \n",
    "                             'Unknown'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approximating 'Company' Values based off of 'text' and 'device_type'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a new column for 'Google' and 'Apple' based on device type and key words in the 'text' column\n",
    "\n",
    "google_key_words = [\"Google\", \"Android\", \"Pixel\", \"Circles\", \"Droid\", \"Galaxy S\", \"Realtime\", \"Maps\", \"Google Maps\", \"Circle\" ]\n",
    "\n",
    "apple_key_words = [\"Apple\", \"iPhone\", \"iPad\", \"Mac\", \"iMac\", \"iPod\", \"iTunes\", \"iWatch\", \"iMessage\", \"iCloud\", \"iBook\", \"iMac\", \"app_store\", \"app store\", \"ios\", \"ios4\", \"ios4.1\", \"ios4.2\", \"iphone app\", \"3g\", \"ios\"]\n",
    "                  \n",
    "df['Google'] = np.where(df['device_type'] == 'Google', True, \n",
    "               np.where(df['text'].str.lower().str.contains('|'.join(google_key_words), case=False), True, \n",
    "               False))\n",
    "\n",
    "df['Apple'] = np.where(df['device_type'] == 'Apple', True,\n",
    "              np.where(df['text'].str.lower().str.contains('|'.join(apple_key_words), case=False), True,\n",
    "              False))\n",
    "\n",
    "### Create new column 'both' that is true if both Google and Apple are true\n",
    "\n",
    "df['both'] = np.where((df['Google'] == True) & (df['Apple'] == True), True, False)\n",
    "\n",
    "### Dropping rows where both Google and Apple are true and where Google and Apple are both false\n",
    "\n",
    "df = df[df['both'] == False]\n",
    "df = df[df['Google'] != df['Apple']]\n",
    "df = df.drop(columns=['both'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Establishing a VADER Sentiment Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating new columns in the dataframe which append 'pos', 'neg', and 'neu' using VADER sentiment analysis\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "df['sentiment'] = df['text'].apply(lambda x: sid.polarity_scores(x))\n",
    "df = pd.concat([df.drop(['sentiment'], axis=1), df['sentiment'].apply(pd.Series)], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating an 'emphasis' column that scores how many exclamation points, question marks, and capital letters are in the text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating an 'emphasis' column that scores how many exclamation points, question marks, and capital letters are in the text\n",
    "\n",
    "df['emphasis'] = df['text'].apply(lambda x: sum([1 for char in x if char in ['!', '?']])) + \\\n",
    "                 df['text'].apply(lambda x: sum([1 for char in x if char.isupper()]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First Simple Model - Count Vectorizer / Decision Tree / No Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy Score: 0.43877872206484103\n",
      "Validation Accuracy Score: 0.46113271836299774\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 78,  38,   0],\n",
       "       [312, 565,  12],\n",
       "       [237, 325,  22]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Performing a train test split on the data, only including the 'text' and 'emotion' columns\n",
    "\n",
    "X1 = df['text']\n",
    "y1 = df['emotion']\n",
    "\n",
    "### Adding the tokenizer to the 'text' column in the X features\n",
    "\n",
    "X1 = X1.apply(clean_text)\n",
    "\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X1, y1, test_size=0.2, random_state=1337)\n",
    "\n",
    "### Creating an imbpalance-learn pipeline that uses SMOTE to oversample the minority classes and then uses a Decision Tree to predict the emotion of a tweet\n",
    "\n",
    "baseline = imbpipeline([\n",
    "    ('cvec', CountVectorizer(encoding = 'iso-8859-1', lowercase = False)),\n",
    "    ('smote', SMOTE(sampling_strategy='minority', random_state=1337)),\n",
    "    ('dt', DecisionTreeClassifier(random_state=1337, max_depth=5))\n",
    "])\n",
    "\n",
    "### Fitting the pipeline to the training data and printing the training and validation accuracy scores\n",
    "\n",
    "baseline.fit(X_train_1, y_train_1)\n",
    "print('Training Accuracy Score:', baseline.score(X_train_1, y_train_1))\n",
    "print('Validation Accuracy Score:', cross_val_score(baseline, X_train_1, y_train_1, cv=5).mean())\n",
    "\n",
    "### Import confusion_matrix and print the confusion matrix for the validation data\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred_1 = baseline.predict(X_test_1)\n",
    "confusion_matrix(y_test_1, y_pred_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>device</th>\n",
       "      <th>emotion</th>\n",
       "      <th>device_type</th>\n",
       "      <th>Google</th>\n",
       "      <th>Apple</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>emphasis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>Apple</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.6800</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>Apple</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.9100</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>Apple</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>Apple</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.7269</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>Google</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.6249</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text              device  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...              iPhone   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...  iPad or iPhone App   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...                iPad   \n",
       "3  @sxsw I hope this year's festival isn't as cra...  iPad or iPhone App   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...              Google   \n",
       "\n",
       "            emotion device_type  Google  Apple    neg    neu    pos  compound  \\\n",
       "0  Negative emotion       Apple   False   True  0.203  0.797  0.000   -0.6800   \n",
       "1  Positive emotion       Apple   False   True  0.000  0.576  0.424    0.9100   \n",
       "2  Positive emotion       Apple   False   True  0.000  1.000  0.000    0.0000   \n",
       "3  Negative emotion       Apple   False   True  0.000  0.663  0.337    0.7269   \n",
       "4  Positive emotion      Google    True  False  0.000  0.796  0.204    0.6249   \n",
       "\n",
       "   emphasis  \n",
       "0        16  \n",
       "1        11  \n",
       "2         7  \n",
       "3         2  \n",
       "4        14  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second Model - Count Vectorizer / Logistic Regression / Added Sentiment and Emphasis Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Performing a train test split on the data, including 'text', VADER scores, and 'emphasis' columns\n",
    "X2 = df.drop(columns=['emotion', 'device', 'device_type', 'Google', 'Apple'])\n",
    "y2 = df['emotion']\n",
    "\n",
    "### Applying the tokenizer to the 'text' column in the X features\n",
    "\n",
    "X2['text'] = X2['text'].apply(clean_text)\n",
    "\n",
    "### Performing a train test split on the X2 and Y2 data\n",
    "\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X2, y2, test_size=0.2, random_state=1337)\n",
    "\n",
    "### Creating an imbpalance-learn pipeline that uses SMOTE to oversample the minority classes and then uses a Logistic Regression to predict the emotion of a tweet\n",
    "\n",
    "# baseline2 = imbpipeline([\n",
    "#     ('cvec', CountVectorizer(encoding = 'iso-8859-1', lowercase = False)),\n",
    "#     ('smote', SMOTE(sampling_strategy='minority', random_state=1337)),\n",
    "#     ('lr', LogisticRegression(random_state=1337, max_iter=1000))\n",
    "# ])\n",
    "\n",
    "\n",
    "\n",
    "### Fitting the pipeline to the training data and printing the training and validation accuracy scores\n",
    "\n",
    "# baseline2.fit(X_train_2, y_train_2)\n",
    "# print('Training Accuracy Score:', baseline2.score(X_train_2, y_train_2))\n",
    "# print('Validation Accuracy Score:', cross_val_score(baseline2, X_train_2, y_train_2, cv=5).mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6354, 6)\n",
      "(6354,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_2.shape)\n",
    "print(y_train_2.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitting the training data to a Logistic Regressions Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_vectorized' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-b87f85514081>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'l2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1337\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_vectorized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m### Cross validating the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_vectorized' is not defined"
     ]
    }
   ],
   "source": [
    "### Fitting the training data to a logistic regression classifier with {'C': 100, 'penalty': 'l2'}\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(C=100, penalty='l2', random_state=1337, max_iter=1000)\n",
    "lr.fit(X_train_vectorized, y_train)\n",
    "\n",
    "### Cross validating the model\n",
    "\n",
    "cross_val_score(lr, X_train_vectorized, y_train, cv=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Data - Transforming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Applying preprocessing to the test data\n",
    "\n",
    "X_test['text'] = X_test['text'].apply(clean_text)\n",
    "X_test['text'] = X_test['text'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "### Transforming the test data\n",
    "\n",
    "X_test_vectorized = tfidf.transform(X_test['text'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Data - Comparing Model Scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fitting the test data to all models developed above\n",
    "\n",
    "rfc_pred = rfc.predict(X_test_vectorized)\n",
    "nb_pred = nb.predict(X_test_vectorized)\n",
    "lr_pred = lr.predict(X_test_vectorized)\n",
    "\n",
    "### Print the accuracy, precision, recall, and f1 score for each model\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('Random Forest Classifier')\n",
    "print('Accuracy: ', accuracy_score(y_test, rfc_pred))\n",
    "print('Precision: ', precision_score(y_test, rfc_pred, average='weighted'))\n",
    "print('Recall: ', recall_score(y_test, rfc_pred, average='weighted'))\n",
    "print('F1 Score: ', f1_score(y_test, rfc_pred, average='weighted'))\n",
    "print('-------------------------------------------------')\n",
    "print('Naive Bayes Classifier')\n",
    "print('Accuracy: ', accuracy_score(y_test, nb_pred))\n",
    "print('Precision: ', precision_score(y_test, nb_pred, average='weighted'))\n",
    "print('Recall: ', recall_score(y_test, nb_pred, average='weighted'))\n",
    "print('F1 Score: ', f1_score(y_test, nb_pred, average='weighted'))\n",
    "print('-------------------------------------------------')\n",
    "print('Logistic Regression Classifier')\n",
    "print('Accuracy: ', accuracy_score(y_test, lr_pred))\n",
    "print('Precision: ', precision_score(y_test, lr_pred, average='weighted'))\n",
    "print('Recall: ', recall_score(y_test, lr_pred, average='weighted'))\n",
    "print('F1 Score: ', f1_score(y_test, lr_pred, average='weighted'))\n",
    "\n",
    "### Print the confusion matrix for each model using seaborn's heatmap\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "sns.heatmap(confusion_matrix(y_test, rfc_pred), annot=True, ax=ax[0], fmt='d')\n",
    "ax[0].set_title('Random Forest Classifier')\n",
    "ax[0].set_xlabel('Predicted')\n",
    "ax[0].set_ylabel('Actual')\n",
    "sns.heatmap(confusion_matrix(y_test, nb_pred), annot=True, ax=ax[1], fmt='d')\n",
    "ax[1].set_title('Naive Bayes Classifier')\n",
    "ax[1].set_xlabel('Predicted')\n",
    "ax[1].set_ylabel('Actual')\n",
    "sns.heatmap(confusion_matrix(y_test, lr_pred), annot=True, ax=ax[2], fmt='d')\n",
    "ax[2].set_title('Logistic Regression Classifier')\n",
    "ax[2].set_xlabel('Predicted')\n",
    "ax[2].set_ylabel('Actual')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "dba4eb4192a401b10630bbfa25b7fb709bd78a83adcb3ebf371257b880947706"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
