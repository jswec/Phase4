{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NLP Over a SXSW Twitter Data Set**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package and Data Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Package Imports*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/diego/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/diego/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "### Packages to Import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import warnings\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Importing CSV File*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Data and Change Column Names\n",
    "\n",
    "df = pd.read_csv(\"data/tweets.csv\")\n",
    "df.columns = ['text', 'device', 'emotion']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Data Cleaning Column Addition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create column that counts the amount of mentions in a tweet\n",
    "0.0 --> 4173 | 1.0 --> 3251 | 2.0 --> 1218 | 3.0 --> 339 | 4.0 --> 80 | 5.0 --> 22 | 6.0 --> 5 | 7.0 --> 2 | 8.0 --> 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mentions'] = df.text.str.count('@')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a column that counts the amount of links in a tweet\n",
    "\n",
    "We are also considering anything with '.com', 'http', 'bit.ly', '.co', and '{link}' as a link\n",
    "\n",
    "0 --> 4807 | 1 --> 4086 | 2 --> 186 | 3 --> 186 | 4 --> 11 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['links'] = 0\n",
    "\n",
    "url_like_strings = ['{link}', '.com', 'http', 'bit.ly', '.co']\n",
    "for s in url_like_strings:\n",
    "    df['links'] = df.links + list(map(lambda x: str(x).count(s), df['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4807\n",
       "1    4068\n",
       "2     186\n",
       "3      21\n",
       "4      11\n",
       "Name: links, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.links.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dropping non-significant values and rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dropping 'I can't tell' and 'Other' rows\n",
    "\n",
    "df = df[df['emotion'] != \"I can't tell\"]\n",
    "\n",
    "### Dropping blank 'text' rows\n",
    "\n",
    "df = df.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating functions that clean the text data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a function that makes all text lowercase for further analysis\n",
    "\n",
    "def lower_case(text):\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "### Creating a function that removes the use of via in the context of via hashtag or via mention (removes 80% of vias)\n",
    "\n",
    "def remove_via(text):\n",
    "    \n",
    "    if 'via @' in text or 'via #' in text:\n",
    "        text = text.replace('via', '')\n",
    "    return text\n",
    "    \n",
    "### Creating a function that removes errant html syntax from the tweet (e.g. &amp; and &quot;)\n",
    "\n",
    "def remove_html(text):\n",
    "    \n",
    "    text = text.replace('&amp;', '')\n",
    "    text = text.replace('&quot;', '')\n",
    "    return text\n",
    "\n",
    "### Creating a function that removes urls or instances of '{link}' from the tweet\n",
    "\n",
    "def remove_url(text):\n",
    "    \n",
    "    url_like_strings = ['{link}', '.com', 'http', 'bit.ly', '.co']\n",
    "    text = text.split()\n",
    "    for s in url_like_strings:\n",
    "        text = [word for word in text if s not in word]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "### Creating a function that removes words that contain a @ and rt (retweet) \n",
    "### as mentions would not be important in determining the emotion of a tweet\n",
    "\n",
    "def remove_at_and_rt(text):\n",
    "    text = text.split()\n",
    "    text = [word for word in text if '@' not in word]\n",
    "    text = [word for word in text if word != 'rt']\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "### Creating a function that removes '#SXSW' of any case type from the text\n",
    "\n",
    "def remove_sxsw(text):\n",
    "    text = text.split()\n",
    "    text = [word for word in text if '#sxsw' not in word]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "### Creating a function that uses a regex tokenizer to remove punctuation but ignores contraction apostrophes\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+\\'?\\w+')\n",
    "    text = tokenizer.tokenize(text)\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "### Creating a function that removes stopwords from a specified list of stopwords\n",
    "\n",
    "custom_stop_words = ['in','of','at','a','the']\n",
    "\n",
    "def remove_stopwords(text, stop_words_list = set(stopwords.words('english'))):\n",
    "    text = text.split()\n",
    "    text = [word for word in text if word not in stop_words_list]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "### Creating a function that removes non-ASCII characters\n",
    "\n",
    "def remove_characters(text):\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    return text\n",
    "\n",
    "### Creating a function that lemmatizes words\n",
    "\n",
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = text.split()\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "### Creating a function that combines all of the above functions\n",
    "\n",
    "def clean_text(text):\n",
    "    text = lower_case(text)\n",
    "    text = remove_via(text)\n",
    "    text = remove_html(text)\n",
    "    text = remove_url(text)\n",
    "    text = remove_at_and_rt(text)\n",
    "    text = remove_sxsw(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = remove_characters(text)\n",
    "    text = lemmatize(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"&quot;via @mention : {link} Guy Kawasaki talks 'Enchanted' at SXSW - HE knows his stuff! #books #internet #Apple #sxsw  &quot;\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guy kawasaki talk enchanted sxsw know stuff book internet apple\n"
     ]
    }
   ],
   "source": [
    "print(clean_text(df['text'][60]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Applying the clean_text function to the 'text' column and creating a santiy check csv\n",
    "\n",
    "df_clean = df.copy()\n",
    "df_clean['text'] = df_clean['text'].apply(clean_text)\n",
    "df_clean.to_csv('data/cleaned_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating 'device_type'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Deliniating between Google and Apple\n",
    "\n",
    "google_tweets = ['Google', 'Other Google product or service', 'Andriod App', 'Andriod']\n",
    "apple_tweets = ['Apple', 'Other Apple product or service', 'Apple App', 'iPhone', 'iPad', 'iPad or iPhone App']\n",
    "\n",
    "### Creating a new column for google vs. apple vs. unknown\n",
    "\n",
    "df['device_type'] = np.where(df['device'].isin(google_tweets), 'Google', \n",
    "                    np.where(df['device'].isin(apple_tweets), 'Apple', \n",
    "                             'Unknown'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approximating 'Company' Values based off of 'text' and 'device_type'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a new column for 'Google' and 'Apple' based on device type and key words in the 'text' column\n",
    "\n",
    "google_key_words = [\"Google\", \"Android\", \"Pixel\", \"Circles\", \"Droid\", \"Galaxy S\", \"Realtime\", \"Maps\", \"Google Maps\", \"Circle\" ]\n",
    "\n",
    "apple_key_words = [\"Apple\", \"iPhone\", \"iPad\", \"Mac\", \"iMac\", \"iPod\", \"iTunes\", \"iWatch\", \"iMessage\", \"iCloud\", \"iBook\", \"iMac\", \n",
    "                   \"app_store\", \"app store\", \"ios\", \"ios4\", \"ios4.1\", \"ios4.2\", \"iphone app\", \"3g\", \"ios\"]\n",
    "                  \n",
    "df['Google'] = np.where(df['device_type'] == 'Google', True, \n",
    "               np.where(df['text'].str.lower().str.contains('|'.join(google_key_words), case=False), True, \n",
    "               False))\n",
    "\n",
    "df['Apple'] = np.where(df['device_type'] == 'Apple', True,\n",
    "              np.where(df['text'].str.lower().str.contains('|'.join(apple_key_words), case=False), True,\n",
    "              False))\n",
    "\n",
    "### Create new column 'both' that is true if both Google and Apple are true\n",
    "\n",
    "df['both'] = np.where((df['Google'] == True) & (df['Apple'] == True), True, False)\n",
    "\n",
    "### Dropping rows where both Google and Apple are true and where Google and Apple are both false\n",
    "\n",
    "df = df[df['both'] == False]\n",
    "df = df[df['Google'] != df['Apple']]\n",
    "df = df.drop(columns=['both'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Establishing a VADER Sentiment Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating new columns in the dataframe which append 'pos', 'neg', and 'neu' using VADER sentiment analysis\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "df['sentiment'] = df['text'].apply(lambda x: sid.polarity_scores(x))\n",
    "df = pd.concat([df.drop(['sentiment'], axis=1), df['sentiment'].apply(pd.Series)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating an 'emphasis' column that scores how many exclamation points, question marks, and capital letters are in the text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating an 'punc_emphasis' column that scores how many exclamation points and question marks are in the text\n",
    "\n",
    "df['punc_emphasis'] = df['text'].apply(lambda x: sum([1 for char in x if char in ['!', '?']]))\n",
    "\n",
    "### Creating a 'capt_emphasis' column that scores how many capitalized words are in the text\n",
    "\n",
    "df['capt_emphasis'] = df['text'].apply(lambda x: sum([1 for word in x.split() if word.isupper()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First Simple Model - Count Vectorizer / Decision Tree / No Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3727480503.py, line 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[17], line 24\u001b[0;36m\u001b[0m\n\u001b[0;31m    plt.figure(figsize=(, 10))\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "### Performing a train test split on the data, only including the 'text' and 'emotion' columns\n",
    "\n",
    "X1 = df['text']\n",
    "y1 = df['emotion']\n",
    "\n",
    "### Adding the tokenizer to the 'text' column in the X features\n",
    "\n",
    "X1 = X1.apply(clean_text)\n",
    "\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X1, y1, test_size=0.2, random_state=1337)\n",
    "\n",
    "\n",
    "### Fitting the pipeline to the training data and printing the training and validation accuracy scores\n",
    "\n",
    "baseline.fit(X_train_1, y_train_1)\n",
    "print('Training Accuracy Score:', baseline.score(X_train_1, y_train_1))\n",
    "print('Validation Accuracy Score:', cross_val_score(baseline, X_train_1, y_train_1, cv=5).mean())\n",
    "\n",
    "### Generate a confusion matrix on the validation data and plot with seaborn\n",
    "\n",
    "y_pred_1 = baseline.predict(X_test_1)\n",
    "cm = confusion_matrix(y_test_1, y_pred_1)\n",
    "cm_df = pd.DataFrame(cm, columns=['Predicted Negative', 'Predicted Neutral', 'Predicted Positive'], index=['Actual Negative', 'Actual Neutral', 'Actual Positive'])\n",
    "plt.figure(figsize=(, 10))\n",
    "sns.heatmap(cm_df, annot=True, fmt='g', cmap='Blues')\n",
    "plt.title('Decision Tree Confusion Matrix')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second Model - Count Vectorizer / Support Vector Machine / Added Sentiment and Emphasis Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Performing a train test split on the data, including 'text', VADER scores, and 'emphasis' columns for the X features\n",
    "\n",
    "X2 = df.drop(columns=['emotion', 'device', 'device_type', 'Google', 'Apple'])\n",
    "y2 = df['emotion']\n",
    "\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X2, y2, test_size=0.2, random_state=1337)\n",
    "\n",
    "### Applying the tokenizer to the 'text' column in the X features\n",
    "\n",
    "X_train_2['text'] = X_train_2['text'].apply(clean_text)\n",
    "\n",
    "### Creating an imbpalance-learn pipeline that uses SMOTE to oversample the minority classes, vectorizes the data, and then uses a Support Vector Machine to predict the emotion of a tweet\n",
    "\n",
    "baseline2 = imbpipeline([\n",
    "    ('smote', SMOTE(sampling_strategy='minority', random_state=1337)),\n",
    "    ('cvec', CountVectorizer(encoding = 'iso-8859-1', lowercase = False)),\n",
    "    ('svm', SVC(random_state=1337))\n",
    "])\n",
    "\n",
    "### Fitting the pipeline to the training data and printing the training and validation accuracy scores\n",
    "\n",
    "baseline2.fit(X_train_2, y_train_2)\n",
    "print('Training Accuracy Score:', baseline2.score(X_train_2, y_train_2))\n",
    "print('Validation Accuracy Score:', cross_val_score(baseline2, X_train_2, y_train_2, cv=5).mean())\n",
    "\n",
    "### Generate a confusion matrix on the validation data and plot with seaborn\n",
    "\n",
    "y_pred_2 = baseline2.predict(X_test_2)\n",
    "cm = confusion_matrix(y_test_2, y_pred_2)\n",
    "cm_df = pd.DataFrame(cm, columns=['Predicted Negative', 'Predicted Neutral', 'Predicted Positive'], index=['Actual Negative', 'Actual Neutral', 'Actual Positive'])\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.heatmap(cm_df, annot=True, fmt='g', cmap='Blues')\n",
    "plt.title('SVM Confusion Matrix')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X_train_2))\n",
    "print(type(y_train_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Data - Transforming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Applying preprocessing to the test data\n",
    "\n",
    "X_test['text'] = X_test['text'].apply(clean_text)\n",
    "X_test['text'] = X_test['text'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "### Transforming the test data\n",
    "\n",
    "X_test_vectorized = tfidf.transform(X_test['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Data - Comparing Model Scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fitting the test data to all models developed above\n",
    "\n",
    "rfc_pred = rfc.predict(X_test_vectorized)\n",
    "nb_pred = nb.predict(X_test_vectorized)\n",
    "lr_pred = lr.predict(X_test_vectorized)\n",
    "\n",
    "### Print the accuracy, precision, recall, and f1 score for each model\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('Random Forest Classifier')\n",
    "print('Accuracy: ', accuracy_score(y_test, rfc_pred))\n",
    "print('Precision: ', precision_score(y_test, rfc_pred, average='weighted'))\n",
    "print('Recall: ', recall_score(y_test, rfc_pred, average='weighted'))\n",
    "print('F1 Score: ', f1_score(y_test, rfc_pred, average='weighted'))\n",
    "print('-------------------------------------------------')\n",
    "print('Naive Bayes Classifier')\n",
    "print('Accuracy: ', accuracy_score(y_test, nb_pred))\n",
    "print('Precision: ', precision_score(y_test, nb_pred, average='weighted'))\n",
    "print('Recall: ', recall_score(y_test, nb_pred, average='weighted'))\n",
    "print('F1 Score: ', f1_score(y_test, nb_pred, average='weighted'))\n",
    "print('-------------------------------------------------')\n",
    "print('Logistic Regression Classifier')\n",
    "print('Accuracy: ', accuracy_score(y_test, lr_pred))\n",
    "print('Precision: ', precision_score(y_test, lr_pred, average='weighted'))\n",
    "print('Recall: ', recall_score(y_test, lr_pred, average='weighted'))\n",
    "print('F1 Score: ', f1_score(y_test, lr_pred, average='weighted'))\n",
    "\n",
    "### Print the confusion matrix for each model using seaborn's heatmap\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "sns.heatmap(confusion_matrix(y_test, rfc_pred), annot=True, ax=ax[0], fmt='d')\n",
    "ax[0].set_title('Random Forest Classifier')\n",
    "ax[0].set_xlabel('Predicted')\n",
    "ax[0].set_ylabel('Actual')\n",
    "sns.heatmap(confusion_matrix(y_test, nb_pred), annot=True, ax=ax[1], fmt='d')\n",
    "ax[1].set_title('Naive Bayes Classifier')\n",
    "ax[1].set_xlabel('Predicted')\n",
    "ax[1].set_ylabel('Actual')\n",
    "sns.heatmap(confusion_matrix(y_test, lr_pred), annot=True, ax=ax[2], fmt='d')\n",
    "ax[2].set_title('Logistic Regression Classifier')\n",
    "ax[2].set_xlabel('Predicted')\n",
    "ax[2].set_ylabel('Actual')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "dba4eb4192a401b10630bbfa25b7fb709bd78a83adcb3ebf371257b880947706"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
