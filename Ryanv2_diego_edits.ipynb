{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NLP Over a SXSW Twitter Data Set**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package and Data Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Package Imports*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/diego/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/diego/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Packages to Import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import warnings\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Importing CSV File*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Data and Change Column Names\n",
    "\n",
    "df = pd.read_csv(\"data/tweets.csv\")\n",
    "df.columns = ['text', 'device', 'emotion']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dropping non-significant values and rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dropping 'I can't tell' and 'Other' rows\n",
    "\n",
    "df = df[df['emotion'] != \"I can't tell\"]\n",
    "\n",
    "### Dropping blank 'text' rows\n",
    "\n",
    "df = df.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating functions that clean the text data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a function that removes words that begin with @, as mentions would not be important in determining the emotion of a tweet\n",
    "\n",
    "def remove_at(text):\n",
    "    text = text.split()\n",
    "    text = [word for word in text if not word.startswith('@')]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "### Creating a function that makes all text lowercase for further analysis\n",
    "\n",
    "def lower_case(text):\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "### Creating a function that removes all punctuation with the exception of ! and ? as they may be important in determining the emotion of a tweet\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    text = text.split()\n",
    "    text = [word for word in text if not word.startswith('!') and not word.startswith('?')]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "### Creating a function that removes stopwords from a specified list of stopwords\n",
    "\n",
    "custom_stop_words = ['in','of','at','a','the']\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = text.split()\n",
    "    text = [word for word in text if word not in custom_stop_words]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "### Creating a function that removes non-ASCII characters\n",
    "\n",
    "def remove_characters(text):\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    return text\n",
    "\n",
    "### Creating a function that lemmatizes words\n",
    "\n",
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = text.split()\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "### Creating a function takes the tokenized text and returns a string of words\n",
    "\n",
    "def tokenize_to_string(text):\n",
    "    tknzr = TweetTokenizer()\n",
    "    text = tknzr.tokenize(text)\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "### Creating a function that removes '#SXSW' of any case type from the text\n",
    "\n",
    "def remove_sxsw(text):\n",
    "    text = text.split()\n",
    "    text = [word for word in text if not word.startswith('#sxsw') and not word.startswith('#SXSW')]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "### Creating a function that combines all of the above functions\n",
    "\n",
    "def clean_text(text):\n",
    "    text = remove_at(text)\n",
    "    text = lower_case(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = remove_characters(text)\n",
    "    text = lemmatize(text)\n",
    "    text = tokenize_to_string(text)\n",
    "    text = remove_sxsw(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating 'device_type'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Deliniating between Google and Apple\n",
    "\n",
    "google_tweets = ['Google', 'Other Google product or service', 'Andriod App', 'Andriod']\n",
    "apple_tweets = ['Apple', 'Other Apple product or service', 'Apple App', 'iPhone', 'iPad', 'iPad or iPhone App']\n",
    "\n",
    "### Creating a new column for google vs. apple vs. unknown\n",
    "\n",
    "df['device_type'] = np.where(df['device'].isin(google_tweets), 'Google', \n",
    "                    np.where(df['device'].isin(apple_tweets), 'Apple', \n",
    "                             'Unknown'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approximating 'Company' Values based off of 'text' and 'device_type'**\n",
    "\n",
    "New dataFrame has 7943 rows\n",
    "\n",
    "Apple: 5241\n",
    "\n",
    "Google: 2702"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a new column for 'Google' and 'Apple' based on device type and key words in the 'text' column\n",
    "\n",
    "google_key_words = [\"Google\", \"Android\", \"Pixel\", \"Circles\", \"Droid\", \"Galaxy S\", \"Realtime\", \"Maps\", \"Google Maps\", \"Circle\" ]\n",
    "\n",
    "apple_key_words = [\"Apple\", \"iPhone\", \"iPad\", \"Mac\", \"iMac\", \"iPod\", \"iTunes\", \"iWatch\", \"iMessage\", \"iCloud\", \"iBook\", \"iMac\", \"app_store\", \"app store\", \"ios\", \"ios4\", \"ios4.1\", \"ios4.2\", \"iphone app\", \"3g\", \"ios\"]\n",
    "                  \n",
    "df['Google'] = np.where(df['device_type'] == 'Google', True, \n",
    "               np.where(df['text'].str.lower().str.contains('|'.join(google_key_words), case=False), True, \n",
    "               False))\n",
    "\n",
    "df['Apple'] = np.where(df['device_type'] == 'Apple', True,\n",
    "              np.where(df['text'].str.lower().str.contains('|'.join(apple_key_words), case=False), True,\n",
    "              False))\n",
    "\n",
    "### Create new column 'both' that is true if both Google and Apple are true\n",
    "\n",
    "df['both'] = np.where((df['Google'] == True) & (df['Apple'] == True), True, False)\n",
    "\n",
    "### Dropping rows where both Google and Apple are true and where Google and Apple are both false\n",
    "\n",
    "df = df[df['both'] == False]\n",
    "df = df[df['Google'] != df['Apple']]\n",
    "df = df.drop(columns=['both'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Establishing a VADER Sentiment Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating new columns in the dataframe which append 'pos', 'neg', and 'neu' using VADER sentiment analysis\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "df['sentiment'] = df['text'].apply(lambda x: sid.polarity_scores(x))\n",
    "df = pd.concat([df.drop(['sentiment'], axis=1), df['sentiment'].apply(pd.Series)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>device</th>\n",
       "      <th>emotion</th>\n",
       "      <th>device_type</th>\n",
       "      <th>Google</th>\n",
       "      <th>Apple</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>Apple</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.6800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>Apple</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.9100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>Apple</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>Apple</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.7269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp;amp; Matt Mullenweg (Wordpress)</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>Google</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.6249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                          text  \\\n",
       "0              .@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW   \n",
       "2                                                              @swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.   \n",
       "3                                                           @sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw   \n",
       "4          @sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp; Matt Mullenweg (Wordpress)   \n",
       "\n",
       "               device           emotion device_type  Google  Apple    neg  \\\n",
       "0              iPhone  Negative emotion       Apple   False   True  0.203   \n",
       "1  iPad or iPhone App  Positive emotion       Apple   False   True  0.000   \n",
       "2                iPad  Positive emotion       Apple   False   True  0.000   \n",
       "3  iPad or iPhone App  Negative emotion       Apple   False   True  0.000   \n",
       "4              Google  Positive emotion      Google    True  False  0.000   \n",
       "\n",
       "     neu    pos  compound  \n",
       "0  0.797  0.000   -0.6800  \n",
       "1  0.576  0.424    0.9100  \n",
       "2  1.000  0.000    0.0000  \n",
       "3  0.663  0.337    0.7269  \n",
       "4  0.796  0.204    0.6249  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating an 'emphasis' column that scores how many exclamation points, question marks, and capital letters are in the text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating an 'emphasis' column that scores how many exclamation points, question marks, and capital letters are in the text\n",
    "\n",
    "df['emphasis'] = df['text'].apply(lambda x: sum([1 for char in x if char in ['!', '?']])) + \\\n",
    "                 df['text'].apply(lambda x: sum([1 for char in x if char.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First Simple Model - Count Vectorizer / Decision Tree / No Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy Score: 0.43877872206484103\n",
      "Validation Accuracy Score: 0.46113271836299774\n",
      "Test Accuracy Score: 0.45938733805527454\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 78,  38,   0],\n",
       "       [312, 565,  12],\n",
       "       [237, 325,  22]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Performing a train test split on the data, only including the 'text' and 'emotion' columns\n",
    "\n",
    "X1 = df['text']\n",
    "y1 = df['emotion']\n",
    "\n",
    "### Adding the tokenizer to the 'text' column in the X features\n",
    "\n",
    "X1 = X1.apply(clean_text)\n",
    "\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X1, y1, test_size=0.2, random_state=1337)\n",
    "\n",
    "### Creating an imbpalance-learn pipeline that uses SMOTE to oversample the minority classes and then uses a Decision Tree to predict the emotion of a tweet\n",
    "\n",
    "baseline = imbpipeline([\n",
    "    ('cvec', CountVectorizer(encoding = 'iso-8859-1', lowercase = False)),\n",
    "    ('smote', SMOTE(sampling_strategy='minority', random_state=1337)),\n",
    "    ('dt', DecisionTreeClassifier(random_state=1337, max_depth=5))\n",
    "])\n",
    "\n",
    "### Fitting the pipeline to the training data and printing the training and validation accuracy scores\n",
    "\n",
    "baseline.fit(X_train_1, y_train_1)\n",
    "print('Training Accuracy Score:', baseline.score(X_train_1, y_train_1))\n",
    "print('Validation Accuracy Score:', cross_val_score(baseline, X_train_1, y_train_1, cv=5).mean())\n",
    "print('Test Accuracy Score:', cross_val_score(baseline, X_test_1, y_test_1, cv=5).mean())\n",
    "\n",
    "### Import confusion_matrix and print the confusion matrix for the validation data\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred_1 = baseline.predict(X_test_1)\n",
    "confusion_matrix(y_test_1, y_pred_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x11225 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 18 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvec = CountVectorizer(encoding = 'iso-8859-1', lowercase = False)\n",
    "t_fit = cvec.fit_transform(df.text)\n",
    "t_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>device</th>\n",
       "      <th>emotion</th>\n",
       "      <th>device_type</th>\n",
       "      <th>Google</th>\n",
       "      <th>Apple</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>emphasis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>Apple</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.6800</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>Apple</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.9100</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>Apple</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>Apple</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.7269</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp;amp; Matt Mullenweg (Wordpress)</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>Google</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.6249</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                          text  \\\n",
       "0              .@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW   \n",
       "2                                                              @swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.   \n",
       "3                                                           @sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw   \n",
       "4          @sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp; Matt Mullenweg (Wordpress)   \n",
       "\n",
       "               device           emotion device_type  Google  Apple    neg  \\\n",
       "0              iPhone  Negative emotion       Apple   False   True  0.203   \n",
       "1  iPad or iPhone App  Positive emotion       Apple   False   True  0.000   \n",
       "2                iPad  Positive emotion       Apple   False   True  0.000   \n",
       "3  iPad or iPhone App  Negative emotion       Apple   False   True  0.000   \n",
       "4              Google  Positive emotion      Google    True  False  0.000   \n",
       "\n",
       "     neu    pos  compound  emphasis  \n",
       "0  0.797  0.000   -0.6800        16  \n",
       "1  0.576  0.424    0.9100        11  \n",
       "2  1.000  0.000    0.0000         7  \n",
       "3  0.663  0.337    0.7269         2  \n",
       "4  0.796  0.204    0.6249        14  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second Model - Count Vectorizer / Logistic Regression / Added Sentiment and Emphasis Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Performing a train test split on the data, including 'text', VADER scores, and 'emphasis' columns\n",
    "X2 = df.drop(columns=['emotion', 'device', 'device_type', 'Google', 'Apple'])\n",
    "y2 = df['emotion']\n",
    "\n",
    "### Applying the tokenizer to the 'text' column in the X features\n",
    "\n",
    "X2['text'] = X2['text'].apply(clean_text)\n",
    "\n",
    "### Performing a train test split on the X2 and Y2 data\n",
    "\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X2, y2, test_size=0.2, random_state=1337)\n",
    "\n",
    "### Creating an imbpalance-learn pipeline that uses SMOTE to oversample the minority classes and then uses a Logistic Regression to predict the emotion of a tweet\n",
    "\n",
    "# baseline2 = imbpipeline([\n",
    "#     ('cvec', CountVectorizer(encoding = 'iso-8859-1', lowercase = False)),\n",
    "#     ('smote', SMOTE(sampling_strategy='minority', random_state=1337)),\n",
    "#     ('lr', LogisticRegression(random_state=1337, max_iter=1000))\n",
    "# ])\n",
    "\n",
    "\n",
    "\n",
    "### Fitting the pipeline to the training data and printing the training and validation accuracy scores\n",
    "\n",
    "# baseline2.fit(X_train_2, y_train_2)\n",
    "# print('Training Accuracy Score:', baseline2.score(X_train_2, y_train_2))\n",
    "# print('Validation Accuracy Score:', cross_val_score(baseline2, X_train_2, y_train_2, cv=5).mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6354, 6)\n",
      "(6354,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_2.shape)\n",
    "print(y_train_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>emphasis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2687</th>\n",
       "      <td>google map app save my life on regular basis rt map ha 150 million users . 40 % user are mobile</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5028</th>\n",
       "      <td>rt google to launch major new social network called circles , possibly today { link }</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3215</th>\n",
       "      <td>nice being able to use just usb charging cord with out plug to add more juice to my iphone . #dfw</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7888</th>\n",
       "      <td>so jealous who is team #android event #androidsxsw . get some swag , girl ! ! !</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.6663</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>google to launch major new social network called circles , possibly today - { link } via</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3739</th>\n",
       "      <td>tried to initiate carpooling ridonkulous taxi line , geek all silent . i bet everyone would do it if there wa an iphone app for it .</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.1027</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>oh snap ! ! ! an 80 party hosted by google !</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.5951</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>brown paper on window a line grows popup apple store by gold's gym sixth &amp; congress . { link }</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>hobo with shotgun iphone game { link }</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3718</th>\n",
       "      <td>google going circle about sn rt we're not launching any product but we're doing plenty else { link }</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6354 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                      text  \\\n",
       "2687                                       google map app save my life on regular basis rt map ha 150 million users . 40 % user are mobile   \n",
       "5028                                                 rt google to launch major new social network called circles , possibly today { link }   \n",
       "3215                                     nice being able to use just usb charging cord with out plug to add more juice to my iphone . #dfw   \n",
       "7888                                                       so jealous who is team #android event #androidsxsw . get some swag , girl ! ! !   \n",
       "796                                               google to launch major new social network called circles , possibly today - { link } via   \n",
       "...                                                                                                                                    ...   \n",
       "3739  tried to initiate carpooling ridonkulous taxi line , geek all silent . i bet everyone would do it if there wa an iphone app for it .   \n",
       "1458                                                                                          oh snap ! ! ! an 80 party hosted by google !   \n",
       "992                                         brown paper on window a line grows popup apple store by gold's gym sixth & congress . { link }   \n",
       "218                                                                                                 hobo with shotgun iphone game { link }   \n",
       "3718                                  google going circle about sn rt we're not launching any product but we're doing plenty else { link }   \n",
       "\n",
       "        neg    neu    pos  compound  emphasis  \n",
       "2687  0.000  1.000  0.000    0.0000         4  \n",
       "5028  0.000  1.000  0.000    0.0000        12  \n",
       "3215  0.000  0.882  0.118    0.4215        11  \n",
       "7888  0.208  0.792  0.000   -0.6663         7  \n",
       "796   0.000  1.000  0.000    0.0000        10  \n",
       "...     ...    ...    ...       ...       ...  \n",
       "3739  0.055  0.945  0.000   -0.1027         3  \n",
       "1458  0.000  0.699  0.301    0.5951        13  \n",
       "992   0.000  1.000  0.000    0.0000        13  \n",
       "218   0.000  1.000  0.000    0.0000        17  \n",
       "3718  0.000  1.000  0.000    0.0000        12  \n",
       "\n",
       "[6354 rows x 6 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitting the training data to a Logistic Regressions Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_vectorized' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-b87f85514081>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'l2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1337\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_vectorized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m### Cross validating the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_vectorized' is not defined"
     ]
    }
   ],
   "source": [
    "### Fitting the training data to a logistic regression classifier with {'C': 100, 'penalty': 'l2'}\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(C=100, penalty='l2', random_state=1337, max_iter=1000)\n",
    "lr.fit(X_train_vectorized, y_train)\n",
    "\n",
    "### Cross validating the model\n",
    "\n",
    "cross_val_score(lr, X_train_vectorized, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Data - Transforming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### Applying preprocessing to the test data\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m X_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mX_test\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(clean_text)\n\u001b[1;32m      4\u001b[0m X_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m X_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(x))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m### Transforming the test data\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "### Applying preprocessing to the test data\n",
    "\n",
    "X_test['text'] = X_test['text'].apply(clean_text)\n",
    "X_test['text'] = X_test['text'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "### Transforming the test data\n",
    "\n",
    "X_test_vectorized = tfidf.transform(X_test['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Data - Comparing Model Scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fitting the test data to all models developed above\n",
    "\n",
    "rfc_pred = rfc.predict(X_test_vectorized)\n",
    "nb_pred = nb.predict(X_test_vectorized)\n",
    "lr_pred = lr.predict(X_test_vectorized)\n",
    "\n",
    "### Print the accuracy, precision, recall, and f1 score for each model\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('Random Forest Classifier')\n",
    "print('Accuracy: ', accuracy_score(y_test, rfc_pred))\n",
    "print('Precision: ', precision_score(y_test, rfc_pred, average='weighted'))\n",
    "print('Recall: ', recall_score(y_test, rfc_pred, average='weighted'))\n",
    "print('F1 Score: ', f1_score(y_test, rfc_pred, average='weighted'))\n",
    "print('-------------------------------------------------')\n",
    "print('Naive Bayes Classifier')\n",
    "print('Accuracy: ', accuracy_score(y_test, nb_pred))\n",
    "print('Precision: ', precision_score(y_test, nb_pred, average='weighted'))\n",
    "print('Recall: ', recall_score(y_test, nb_pred, average='weighted'))\n",
    "print('F1 Score: ', f1_score(y_test, nb_pred, average='weighted'))\n",
    "print('-------------------------------------------------')\n",
    "print('Logistic Regression Classifier')\n",
    "print('Accuracy: ', accuracy_score(y_test, lr_pred))\n",
    "print('Precision: ', precision_score(y_test, lr_pred, average='weighted'))\n",
    "print('Recall: ', recall_score(y_test, lr_pred, average='weighted'))\n",
    "print('F1 Score: ', f1_score(y_test, lr_pred, average='weighted'))\n",
    "\n",
    "### Print the confusion matrix for each model using seaborn's heatmap\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "sns.heatmap(confusion_matrix(y_test, rfc_pred), annot=True, ax=ax[0], fmt='d')\n",
    "ax[0].set_title('Random Forest Classifier')\n",
    "ax[0].set_xlabel('Predicted')\n",
    "ax[0].set_ylabel('Actual')\n",
    "sns.heatmap(confusion_matrix(y_test, nb_pred), annot=True, ax=ax[1], fmt='d')\n",
    "ax[1].set_title('Naive Bayes Classifier')\n",
    "ax[1].set_xlabel('Predicted')\n",
    "ax[1].set_ylabel('Actual')\n",
    "sns.heatmap(confusion_matrix(y_test, lr_pred), annot=True, ax=ax[2], fmt='d')\n",
    "ax[2].set_title('Logistic Regression Classifier')\n",
    "ax[2].set_xlabel('Predicted')\n",
    "ax[2].set_ylabel('Actual')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "dba4eb4192a401b10630bbfa25b7fb709bd78a83adcb3ebf371257b880947706"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
