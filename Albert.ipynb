{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project Submission\n",
    "\n",
    "Please fill out:\n",
    "* Student name: \n",
    "* Student pace: self paced / part time / full time\n",
    "* Scheduled project review date/time: \n",
    "* Instructor name: \n",
    "* Blog post URL:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/albertcc/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/albertcc/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, recall_score, precision_score\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from wordcloud import WordCloud\n",
    "import warnings\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/tweets.csv')\n",
    "data.columns = ['text', 'device', 'emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>device</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text              device  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...              iPhone   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...  iPad or iPhone App   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...                iPad   \n",
       "3  @sxsw I hope this year's festival isn't as cra...  iPad or iPhone App   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...              Google   \n",
       "\n",
       "            emotion  \n",
       "0  Negative emotion  \n",
       "1  Positive emotion  \n",
       "2  Positive emotion  \n",
       "3  Negative emotion  \n",
       "4  Positive emotion  "
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9093 entries, 0 to 9092\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   text     9092 non-null   object\n",
      " 1   device   3291 non-null   object\n",
      " 2   emotion  9093 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 213.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No emotion toward brand or product    5389\n",
       "Positive emotion                      2978\n",
       "Negative emotion                       570\n",
       "I can't tell                           156\n",
       "Name: emotion, dtype: int64"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Data Cleaning Column Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a column that counts the amount of mentions in each tweet\n",
    "\n",
    "data['mentions'] = data.text.str.count('@')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a column that counts teh amount of links in a tweet\n",
    "data['links'] = 0\n",
    "\n",
    "url_like_strings = ['{link}', '.com', 'http', 'bit.ly', '.co']\n",
    "for s in url_like_strings:\n",
    "    data['links'] = data.links + list(map(lambda x: str(x).count(s), data['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dropping 'I can't tell' and 'Other' rows\n",
    "\n",
    "data = data[data['emotion'] != \"I can't tell\"]\n",
    "\n",
    "### Dropping blank 'text' rows\n",
    "\n",
    "data = data.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No emotion toward brand or product    5388\n",
       "Positive emotion                      2978\n",
       "Negative emotion                       570\n",
       "Name: emotion, dtype: int64"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iPad                               0.287020\n",
       "Apple                              0.200792\n",
       "iPad or iPhone App                 0.143205\n",
       "Google                             0.130713\n",
       "iPhone                             0.090189\n",
       "Other Google product or service    0.088970\n",
       "Android App                        0.024680\n",
       "Android                            0.023766\n",
       "Other Apple product or service     0.010664\n",
       "Name: device, dtype: float64"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['device'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean text data using functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a function that makes all text lowercase for further analysis\n",
    "\n",
    "def lower_case(text):\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "### Creating a function that removes the use of via in the context of via hashtag or via mention (removes 80% of vias)\n",
    "\n",
    "def remove_via(text):\n",
    "    \n",
    "    if 'via @' in text or 'via #' in text:\n",
    "        text = text.replace('via', '')\n",
    "    return text\n",
    "    \n",
    "### Creating a function that removes errant html syntax from the tweet (e.g. &amp; and &quot;)\n",
    "\n",
    "def remove_html(text):\n",
    "    \n",
    "    text = text.replace('&amp;', '')\n",
    "    text = text.replace('&quot;', '')\n",
    "    return text\n",
    "\n",
    "### Creating a function that removes urls or instances of '{link}' from the tweet\n",
    "\n",
    "def remove_url(text):\n",
    "    \n",
    "    url_like_strings = ['{link}', '.com', 'http', 'bit.ly', '.co']\n",
    "    text = text.split()\n",
    "    for s in url_like_strings:\n",
    "        text = [word for word in text if s not in word]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "### Creating a function that removes words that contain a @ and rt (retweet) \n",
    "### as mentions would not be important in determining the emotion of a tweet\n",
    "\n",
    "def remove_at_and_rt(text):\n",
    "    text = text.split()\n",
    "    text = [word for word in text if '@' not in word]\n",
    "    text = [word for word in text if word != 'rt']\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "### Creating a function that removes '#SXSW' of any case type from the text\n",
    "\n",
    "def remove_sxsw(text):\n",
    "    text = text.split()\n",
    "    text = [word for word in text if '#sxsw' not in word]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "### Creating a function that uses a regex tokenizer to remove punctuation but ignores contraction apostrophes\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+\\'?\\w+')\n",
    "    text = tokenizer.tokenize(text)\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "### Creating a function that removes stopwords from a specified list of stopwords\n",
    "\n",
    "custom_stop_words = ['in','of','at','a','the']\n",
    "\n",
    "def remove_stopwords(text, stop_words_list = set(stopwords.words('english'))):\n",
    "    text = text.split()\n",
    "    text = [word for word in text if word not in stop_words_list]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "### Creating a function that removes non-ASCII characters\n",
    "\n",
    "def remove_characters(text):\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    return text\n",
    "\n",
    "### Creating a function that lemmatizes words\n",
    "\n",
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = text.split()\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "### Creating a function that combines all of the above functions\n",
    "\n",
    "def clean_text(text):\n",
    "    text = lower_case(text)\n",
    "    text = remove_via(text)\n",
    "    text = remove_html(text)\n",
    "    text = remove_url(text)\n",
    "    text = remove_at_and_rt(text)\n",
    "    text = remove_sxsw(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = remove_characters(text)\n",
    "    text = lemmatize(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello test text sb eee'"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Testing out the function with some random text\n",
    "clean_text('Hello this is the test text THAT I have #sb #sxsw eee @!twitter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&quot;via @mention : {link} Guy Kawasaki talks 'Enchanted' at SXSW - HE knows his stuff! #books #internet #Apple #sxsw  &quot;\n",
      "guy kawasaki talk enchanted sxsw know stuff book internet apple\n"
     ]
    }
   ],
   "source": [
    "### Additional test of before and after\n",
    "print(data.text[60])\n",
    "print(clean_text(data['text'][60]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple = ['iPad', 'Apple', 'iPad or iPhone App', 'iPhone', 'Other Apple product or service']\n",
    "google = ['Google', 'Other Google product or service', 'Android App', 'Android']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a new column for google vs. apple vs. unknown\n",
    "\n",
    "data['device_type'] = np.where(data['device'].isin(google), 'Google', \n",
    "                    np.where(data['device'].isin(apple), 'Apple', \n",
    "                             'Unknown'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a new column for 'Google' and 'Apple' based on device type and key words in the 'text' column\n",
    "\n",
    "google_key_words = [\"Google\", \"Android\", \"Pixel\", \"Circles\", \"Droid\", \"Galaxy S\", \"Realtime\", \"Maps\", \"Google Maps\", \"Circle\" ]\n",
    "\n",
    "apple_key_words = [\"Apple\", \"iPhone\", \"iPad\", \"Mac\", \"iMac\", \"iPod\", \"iTunes\", \"iWatch\", \"iMessage\", \"iCloud\", \"iBook\", \"iMac\", \"app_store\", \"app store\", \"ios\", \"ios4\", \"ios4.1\", \"ios4.2\", \"iphone app\", \"3g\", \"ios\"]\n",
    "                  \n",
    "data['Google'] = np.where(data['device_type'] == 'Google', True, \n",
    "               np.where(data['text'].str.lower().str.contains('|'.join(google_key_words), case=False), True, \n",
    "               False))\n",
    "\n",
    "data['Apple'] = np.where(data['device_type'] == 'Apple', True,\n",
    "              np.where(data['text'].str.lower().str.contains('|'.join(apple_key_words), case=False), True,\n",
    "              False))\n",
    "\n",
    "### Create new column 'both' that is true if both Google and Apple are true\n",
    "\n",
    "data['both'] = np.where((data['Google'] == True) & (data['Apple'] == True), True, False)\n",
    "\n",
    "### Dropping rows where both Google and Apple are true and where Google and Apple are both false\n",
    "\n",
    "data = data[data['both'] == False]\n",
    "data = data[data['Google'] != data['Apple']]\n",
    "data = data.drop(columns=['both'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>device</th>\n",
       "      <th>emotion</th>\n",
       "      <th>mentions</th>\n",
       "      <th>links</th>\n",
       "      <th>device_type</th>\n",
       "      <th>Google</th>\n",
       "      <th>Apple</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Apple</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Apple</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Apple</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Apple</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Google</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9088</th>\n",
       "      <td>Ipad everywhere. #SXSW {link}</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Apple</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9089</th>\n",
       "      <td>Wave, buzz... RT @mention We interrupt your re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9090</th>\n",
       "      <td>Google's Zeiger, a physician never reported po...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9091</th>\n",
       "      <td>Some Verizon iPhone customers complained their...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9092</th>\n",
       "      <td>ŒÏ¡ŽÏàŠü_‹Ê‹Î‹Ò‹£‹Á‹ââ‹_‹£‹‹â_‹ÛâRT @...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7943 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text              device  \\\n",
       "0     .@wesley83 I have a 3G iPhone. After 3 hrs twe...              iPhone   \n",
       "1     @jessedee Know about @fludapp ? Awesome iPad/i...  iPad or iPhone App   \n",
       "2     @swonderlin Can not wait for #iPad 2 also. The...                iPad   \n",
       "3     @sxsw I hope this year's festival isn't as cra...  iPad or iPhone App   \n",
       "4     @sxtxstate great stuff on Fri #SXSW: Marissa M...              Google   \n",
       "...                                                 ...                 ...   \n",
       "9088                      Ipad everywhere. #SXSW {link}                iPad   \n",
       "9089  Wave, buzz... RT @mention We interrupt your re...                 NaN   \n",
       "9090  Google's Zeiger, a physician never reported po...                 NaN   \n",
       "9091  Some Verizon iPhone customers complained their...                 NaN   \n",
       "9092  ŒÏ¡ŽÏàŠü_‹Ê‹Î‹Ò‹£‹Á‹ââ‹_‹£‹‹â_‹ÛâRT @...                 NaN   \n",
       "\n",
       "                                 emotion  mentions  links device_type  Google  \\\n",
       "0                       Negative emotion       1.0      0       Apple   False   \n",
       "1                       Positive emotion       2.0      0       Apple   False   \n",
       "2                       Positive emotion       1.0      0       Apple   False   \n",
       "3                       Negative emotion       1.0      0       Apple   False   \n",
       "4                       Positive emotion       1.0      0      Google    True   \n",
       "...                                  ...       ...    ...         ...     ...   \n",
       "9088                    Positive emotion       0.0      1       Apple   False   \n",
       "9089  No emotion toward brand or product       1.0      1     Unknown    True   \n",
       "9090  No emotion toward brand or product       0.0      0     Unknown    True   \n",
       "9091  No emotion toward brand or product       0.0      0     Unknown   False   \n",
       "9092  No emotion toward brand or product       1.0      1     Unknown    True   \n",
       "\n",
       "      Apple  \n",
       "0      True  \n",
       "1      True  \n",
       "2      True  \n",
       "3      True  \n",
       "4     False  \n",
       "...     ...  \n",
       "9088   True  \n",
       "9089  False  \n",
       "9090  False  \n",
       "9091   True  \n",
       "9092  False  \n",
       "\n",
       "[7943 rows x 8 columns]"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive emotion                      1898\n",
       "Negative emotion                       381\n",
       "No emotion toward brand or product      65\n",
       "Name: emotion, dtype: int64"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['device_type']=='Apple']['emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive emotion                      692\n",
       "Negative emotion                      122\n",
       "No emotion toward brand or product     26\n",
       "Name: emotion, dtype: int64"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['device_type']=='Google']['emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vader Score sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating new columns in the dataframe which append 'pos', 'neg', and 'neu' using VADER sentiment analysis\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "data['sentiment'] = data['text'].apply(lambda x: sid.polarity_scores(x))\n",
    "data = pd.concat([data.drop(['sentiment'], axis=1), data['sentiment'].apply(pd.Series)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating an 'emphasis' column that scores how many exclamation points, question marks, and capital letters are in the text\n",
    "\n",
    "data['punc_emphasis'] = data['text'].apply(lambda x: sum([1 for char in x if char in ['!', '?']]))\n",
    "data['capt_emphasis'] = data['text'].apply(lambda x: sum([1 for char in x if char.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>device</th>\n",
       "      <th>emotion</th>\n",
       "      <th>mentions</th>\n",
       "      <th>links</th>\n",
       "      <th>device_type</th>\n",
       "      <th>Google</th>\n",
       "      <th>Apple</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>punc_emphasis</th>\n",
       "      <th>capt_emphasis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Apple</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.6800</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Apple</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.9100</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Apple</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Apple</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.7269</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Google</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.6249</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9088</th>\n",
       "      <td>Ipad everywhere. #SXSW {link}</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Apple</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9089</th>\n",
       "      <td>Wave, buzz... RT @mention We interrupt your re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.4939</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9090</th>\n",
       "      <td>Google's Zeiger, a physician never reported po...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9091</th>\n",
       "      <td>Some Verizon iPhone customers complained their...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.891</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.4019</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9092</th>\n",
       "      <td>ŒÏ¡ŽÏàŠü_‹Ê‹Î‹Ò‹£‹Á‹ââ‹_‹£‹‹â_‹ÛâRT @...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7943 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text              device  \\\n",
       "0     .@wesley83 I have a 3G iPhone. After 3 hrs twe...              iPhone   \n",
       "1     @jessedee Know about @fludapp ? Awesome iPad/i...  iPad or iPhone App   \n",
       "2     @swonderlin Can not wait for #iPad 2 also. The...                iPad   \n",
       "3     @sxsw I hope this year's festival isn't as cra...  iPad or iPhone App   \n",
       "4     @sxtxstate great stuff on Fri #SXSW: Marissa M...              Google   \n",
       "...                                                 ...                 ...   \n",
       "9088                      Ipad everywhere. #SXSW {link}                iPad   \n",
       "9089  Wave, buzz... RT @mention We interrupt your re...                 NaN   \n",
       "9090  Google's Zeiger, a physician never reported po...                 NaN   \n",
       "9091  Some Verizon iPhone customers complained their...                 NaN   \n",
       "9092  ŒÏ¡ŽÏàŠü_‹Ê‹Î‹Ò‹£‹Á‹ââ‹_‹£‹‹â_‹ÛâRT @...                 NaN   \n",
       "\n",
       "                                 emotion  mentions  links device_type  Google  \\\n",
       "0                       Negative emotion       1.0      0       Apple   False   \n",
       "1                       Positive emotion       2.0      0       Apple   False   \n",
       "2                       Positive emotion       1.0      0       Apple   False   \n",
       "3                       Negative emotion       1.0      0       Apple   False   \n",
       "4                       Positive emotion       1.0      0      Google    True   \n",
       "...                                  ...       ...    ...         ...     ...   \n",
       "9088                    Positive emotion       0.0      1       Apple   False   \n",
       "9089  No emotion toward brand or product       1.0      1     Unknown    True   \n",
       "9090  No emotion toward brand or product       0.0      0     Unknown    True   \n",
       "9091  No emotion toward brand or product       0.0      0     Unknown   False   \n",
       "9092  No emotion toward brand or product       1.0      1     Unknown    True   \n",
       "\n",
       "      Apple    neg    neu    pos  compound  punc_emphasis  capt_emphasis  \n",
       "0      True  0.203  0.797  0.000   -0.6800              1             15  \n",
       "1      True  0.000  0.576  0.424    0.9100              1             10  \n",
       "2      True  0.000  1.000  0.000    0.0000              0              7  \n",
       "3      True  0.000  0.663  0.337    0.7269              0              2  \n",
       "4     False  0.000  0.796  0.204    0.6249              0             14  \n",
       "...     ...    ...    ...    ...       ...            ...            ...  \n",
       "9088   True  0.000  1.000  0.000    0.0000              0              5  \n",
       "9089  False  0.208  0.792  0.000   -0.4939              0              4  \n",
       "9090  False  0.000  1.000  0.000    0.0000              0              9  \n",
       "9091   True  0.109  0.891  0.000   -0.4019              0             10  \n",
       "9092  False  0.000  1.000  0.000    0.0000              0             24  \n",
       "\n",
       "[7943 rows x 14 columns]"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>device</th>\n",
       "      <th>emotion</th>\n",
       "      <th>mentions</th>\n",
       "      <th>links</th>\n",
       "      <th>device_type</th>\n",
       "      <th>Google</th>\n",
       "      <th>Apple</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>punc_emphasis</th>\n",
       "      <th>capt_emphasis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Apple</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.9100</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Apple</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.7269</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Google</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.6249</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#SXSW is just starting, #CTIA is around the co...</td>\n",
       "      <td>Android</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Google</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.6369</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Beautifully smart and simple idea RT @madebyma...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Apple</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.691</td>\n",
       "      <td>0.309</td>\n",
       "      <td>0.7712</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9072</th>\n",
       "      <td>@mention your iPhone 4 cases are Rad and Ready...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Apple</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.752</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.2225</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9073</th>\n",
       "      <td>At #SXSW your iphone charger is your best friend.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.486</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0.8126</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9077</th>\n",
       "      <td>@mention your PR guy just convinced me to swit...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Apple</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.673</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.7783</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9079</th>\n",
       "      <td>&amp;quot;papyrus...sort of like the ipad&amp;quot; - ...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Apple</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.409</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.8264</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9080</th>\n",
       "      <td>Diller says Google TV &amp;quot;might be run over ...</td>\n",
       "      <td>Other Google product or service</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Google</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.889</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3360 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "1     @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "3     @sxsw I hope this year's festival isn't as cra...   \n",
       "4     @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "7     #SXSW is just starting, #CTIA is around the co...   \n",
       "8     Beautifully smart and simple idea RT @madebyma...   \n",
       "...                                                 ...   \n",
       "9072  @mention your iPhone 4 cases are Rad and Ready...   \n",
       "9073  At #SXSW your iphone charger is your best friend.   \n",
       "9077  @mention your PR guy just convinced me to swit...   \n",
       "9079  &quot;papyrus...sort of like the ipad&quot; - ...   \n",
       "9080  Diller says Google TV &quot;might be run over ...   \n",
       "\n",
       "                               device                             emotion  \\\n",
       "1                  iPad or iPhone App                    Positive emotion   \n",
       "3                  iPad or iPhone App                    Negative emotion   \n",
       "4                              Google                    Positive emotion   \n",
       "7                             Android                    Positive emotion   \n",
       "8                  iPad or iPhone App                    Positive emotion   \n",
       "...                               ...                                 ...   \n",
       "9072                           iPhone                    Positive emotion   \n",
       "9073                              NaN  No emotion toward brand or product   \n",
       "9077                           iPhone                    Positive emotion   \n",
       "9079                             iPad                    Positive emotion   \n",
       "9080  Other Google product or service                    Negative emotion   \n",
       "\n",
       "      mentions  links device_type  Google  Apple    neg    neu    pos  \\\n",
       "1          2.0      0       Apple   False   True  0.000  0.576  0.424   \n",
       "3          1.0      0       Apple   False   True  0.000  0.663  0.337   \n",
       "4          1.0      0      Google    True  False  0.000  0.796  0.204   \n",
       "7          0.0      0      Google    True  False  0.000  0.822  0.178   \n",
       "8          2.0      2       Apple   False   True  0.000  0.691  0.309   \n",
       "...        ...    ...         ...     ...    ...    ...    ...    ...   \n",
       "9072       1.0      1       Apple   False   True  0.103  0.752  0.145   \n",
       "9073       0.0      0     Unknown   False   True  0.000  0.486  0.514   \n",
       "9077       1.0      0       Apple   False   True  0.000  0.673  0.327   \n",
       "9079       0.0      0       Apple   False   True  0.000  0.409  0.591   \n",
       "9080       0.0      0      Google    True  False  0.000  0.889  0.111   \n",
       "\n",
       "      compound  punc_emphasis  capt_emphasis  \n",
       "1       0.9100              1             10  \n",
       "3       0.7269              0              2  \n",
       "4       0.6249              0             14  \n",
       "7       0.6369              0              8  \n",
       "8       0.7712              1              7  \n",
       "...        ...            ...            ...  \n",
       "9072    0.2225              2              5  \n",
       "9073    0.8126              0              5  \n",
       "9077    0.7783              0              4  \n",
       "9079    0.8264              2              6  \n",
       "9080    0.3612              0              7  \n",
       "\n",
       "[3360 rows x 14 columns]"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['pos'] > 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>device</th>\n",
       "      <th>emotion</th>\n",
       "      <th>mentions</th>\n",
       "      <th>links</th>\n",
       "      <th>device_type</th>\n",
       "      <th>Google</th>\n",
       "      <th>Apple</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>punc_emphasis</th>\n",
       "      <th>capt_emphasis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Apple</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.9100</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Apple</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Google</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.6249</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#SXSW is just starting, #CTIA is around the co...</td>\n",
       "      <td>Android</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Google</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.6369</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Beautifully smart and simple idea RT @madebyma...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Apple</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.691</td>\n",
       "      <td>0.309</td>\n",
       "      <td>0.7712</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9072</th>\n",
       "      <td>@mention your iPhone 4 cases are Rad and Ready...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Apple</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.752</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.2225</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9077</th>\n",
       "      <td>@mention your PR guy just convinced me to swit...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Apple</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.673</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.7783</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9079</th>\n",
       "      <td>&amp;quot;papyrus...sort of like the ipad&amp;quot; - ...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Apple</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.409</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.8264</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9085</th>\n",
       "      <td>I've always used Camera+ for my iPhone b/c it ...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Apple</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9088</th>\n",
       "      <td>Ipad everywhere. #SXSW {link}</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Apple</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2874 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text              device  \\\n",
       "1     @jessedee Know about @fludapp ? Awesome iPad/i...  iPad or iPhone App   \n",
       "2     @swonderlin Can not wait for #iPad 2 also. The...                iPad   \n",
       "4     @sxtxstate great stuff on Fri #SXSW: Marissa M...              Google   \n",
       "7     #SXSW is just starting, #CTIA is around the co...             Android   \n",
       "8     Beautifully smart and simple idea RT @madebyma...  iPad or iPhone App   \n",
       "...                                                 ...                 ...   \n",
       "9072  @mention your iPhone 4 cases are Rad and Ready...              iPhone   \n",
       "9077  @mention your PR guy just convinced me to swit...              iPhone   \n",
       "9079  &quot;papyrus...sort of like the ipad&quot; - ...                iPad   \n",
       "9085  I've always used Camera+ for my iPhone b/c it ...  iPad or iPhone App   \n",
       "9088                      Ipad everywhere. #SXSW {link}                iPad   \n",
       "\n",
       "               emotion  mentions  links device_type  Google  Apple    neg  \\\n",
       "1     Positive emotion       2.0      0       Apple   False   True  0.000   \n",
       "2     Positive emotion       1.0      0       Apple   False   True  0.000   \n",
       "4     Positive emotion       1.0      0      Google    True  False  0.000   \n",
       "7     Positive emotion       0.0      0      Google    True  False  0.000   \n",
       "8     Positive emotion       2.0      2       Apple   False   True  0.000   \n",
       "...                ...       ...    ...         ...     ...    ...    ...   \n",
       "9072  Positive emotion       1.0      1       Apple   False   True  0.103   \n",
       "9077  Positive emotion       1.0      0       Apple   False   True  0.000   \n",
       "9079  Positive emotion       0.0      0       Apple   False   True  0.000   \n",
       "9085  Positive emotion       0.0      0       Apple   False   True  0.000   \n",
       "9088  Positive emotion       0.0      1       Apple   False   True  0.000   \n",
       "\n",
       "        neu    pos  compound  punc_emphasis  capt_emphasis  \n",
       "1     0.576  0.424    0.9100              1             10  \n",
       "2     1.000  0.000    0.0000              0              7  \n",
       "4     0.796  0.204    0.6249              0             14  \n",
       "7     0.822  0.178    0.6369              0              8  \n",
       "8     0.691  0.309    0.7712              1              7  \n",
       "...     ...    ...       ...            ...            ...  \n",
       "9072  0.752  0.145    0.2225              2              5  \n",
       "9077  0.673  0.327    0.7783              0              4  \n",
       "9079  0.409  0.591    0.8264              2              6  \n",
       "9085  1.000  0.000    0.0000              1             13  \n",
       "9088  1.000  0.000    0.0000              0              5  \n",
       "\n",
       "[2874 rows x 14 columns]"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['emotion'] == \"Positive emotion\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Performing a train/test split\n",
    "\n",
    "X = data.drop('emotion', axis=1)\n",
    "y = data['emotion']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Simple Model - Count Vectorizer / Decision Tree / No Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Performing a train test split on the data, only including the 'text' and 'emotion' columns\n",
    "\n",
    "X1 = data['text']\n",
    "y1 = data['emotion']\n",
    "\n",
    "### Adding the tokenizer to the 'text' column in the X features\n",
    "\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X1, y1, test_size=0.2, random_state=1337)\n",
    "\n",
    "X_train_1 = X_train_1.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2687    google map app save life regular basis map 150...\n",
       "5028    google launch major new social network called ...\n",
       "3215    nice able use usb charging cord plug add juice...\n",
       "7888    jealous team android event androidsxsw get swa...\n",
       "796     google launch major new social network called ...\n",
       "                              ...                        \n",
       "3739    tried initiate carpooling ridonkulous taxi lin...\n",
       "1458                       oh snap 80 party hosted google\n",
       "992     brown paper window line grows popup apple stor...\n",
       "218                              hobo shotgun iphone game\n",
       "3718    google going circle sn we're launching product...\n",
       "Name: text, Length: 6354, dtype: object"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3g iphone hr tweeting rise_austin dead need upgrade plugin station'"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3042    Mayer comes out sans intro, still gets cheers....\n",
       "6281    RT @mention Love it. at #sxsw: &quot;apple com...\n",
       "5930    RT @mention Google's Marissa Mayer extolling c...\n",
       "1759    I'll pay $900 for a new iPad 2, white, 32 GB, ...\n",
       "5569    RT @mention Best thing I've heard this wknd @m...\n",
       "                              ...                        \n",
       "3322    so who's gonna be getting an ipad2 from the ap...\n",
       "8089    the longest line at #sxsw is at the apple pop ...\n",
       "913     Hm? Do we need another 1? RT @mention Google t...\n",
       "4733    Apple plans to Keep Austin Wired, opening a po...\n",
       "8308    Talk about trying to steal the show... RT @men...\n",
       "Name: text, Length: 1589, dtype: object"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Mayer comes out sans intro, still gets cheers. #techrockstar Launches into Google's priority on location - Fast, Fun &amp; Future #sxsw\""
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_1[3042]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores:  [0.65460268 0.64358773 0.63808025 0.63099921 0.64094488]\n",
      "Mean Cross Validation Score:  0.6416429496273627\n"
     ]
    }
   ],
   "source": [
    "### TRAIN - Tokenize the training data with a simple split of words, and then flattening to prepare for vectorization\n",
    "\n",
    "X_train_1 = X_train_1.apply(lambda x: x.split())\n",
    "X_train_1 = X_train_1.map(' '.join)\n",
    "\n",
    "### TRAIN - Vectorize the training data using CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X_train_1 = cv.fit_transform(X_train_1)\n",
    "\n",
    "### Insert Sampling Technique\n",
    "\n",
    "sm = SMOTE(random_state=1337)\n",
    "X_train_1_res, y_train_1_res = sm.fit_resample(X_train_1, y_train_1)\n",
    "\n",
    "### TRAIN - Fit the training data to a Decision Tree Classifier\n",
    "\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(X_train_1_res, y_train_1_res)\n",
    "\n",
    "### VALIDATION - Perform a cross validation on the decision tree classifier\n",
    "\n",
    "scores = cross_val_score(dtc, X_train_1, y_train_1, cv=5)\n",
    "print('Cross Validation Scores: ', scores)\n",
    "print('Mean Cross Validation Score: ', scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Decision Tree Test Set Preprocessing\n",
    "\n",
    "X_test_1 = X_test_1.apply(clean_text)\n",
    "X_test_1 = X_test_1.apply(lambda x: x.split())\n",
    "X_test_1 = X_test_1.map(' '.join)\n",
    "X_test_1 = cv.transform(X_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_pred = dtc.predict(X_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy:  0.5475141598489616\n"
     ]
    }
   ],
   "source": [
    "print('Decision Tree Accuracy: ', accuracy_score(y_test_1, dtc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10893x7122 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 111381 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_1_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7943,)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7943,)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6354, 7122)"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6354,)"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    .@wesley83 I have a 3G iPhone. After 3 hrs twe...\n",
       "1    @jessedee Know about @fludapp ? Awesome iPad/i...\n",
       "2    @swonderlin Can not wait for #iPad 2 also. The...\n",
       "3    @sxsw I hope this year's festival isn't as cra...\n",
       "4    @sxtxstate great stuff on Fri #SXSW: Marissa M...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Negative emotion\n",
       "1    Positive emotion\n",
       "2    Positive emotion\n",
       "3    Negative emotion\n",
       "4    Positive emotion\n",
       "Name: emotion, dtype: object"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second Model - Count Vectorizer / Logistic Regression / Added Sentiment and Emphasis Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-266-85eed35495f8>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train_2['text'] = X_train_2['text'].apply(clean_text)\n"
     ]
    }
   ],
   "source": [
    "### Performing a train test split on the data, including 'text', VADER scores, and 'emphasis' columns\n",
    "X2 = data[['text', 'compound', 'neg', 'neu', 'pos', 'punc_emphasis', 'capt_emphasis']]\n",
    "y2 = data['emotion']\n",
    "\n",
    "### Performing a train test split on the X2 and Y2 data\n",
    "\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X2, y2, test_size=0.2, random_state=1337)\n",
    "\n",
    "\n",
    "### Applying the tokenizer to the 'text' column in the X features\n",
    "\n",
    "X_train_2['text'] = X_train_2['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>compound</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>punc_emphasis</th>\n",
       "      <th>capt_emphasis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2687</th>\n",
       "      <td>google map app save life regular basis map 150...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5028</th>\n",
       "      <td>google launch major new social network called ...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3215</th>\n",
       "      <td>nice able use usb charging cord plug add juice...</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7888</th>\n",
       "      <td>jealous team android event androidsxsw get swa...</td>\n",
       "      <td>-0.6663</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>google launch major new social network called ...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3739</th>\n",
       "      <td>tried initiate carpooling ridonkulous taxi lin...</td>\n",
       "      <td>-0.1027</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>oh snap 80 party hosted google</td>\n",
       "      <td>0.5951</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.301</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>brown paper window line grows popup apple stor...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>hobo shotgun iphone game</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3718</th>\n",
       "      <td>google going circle sn we're launching product...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6354 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  compound    neg  \\\n",
       "2687  google map app save life regular basis map 150...    0.0000  0.000   \n",
       "5028  google launch major new social network called ...    0.0000  0.000   \n",
       "3215  nice able use usb charging cord plug add juice...    0.4215  0.000   \n",
       "7888  jealous team android event androidsxsw get swa...   -0.6663  0.208   \n",
       "796   google launch major new social network called ...    0.0000  0.000   \n",
       "...                                                 ...       ...    ...   \n",
       "3739  tried initiate carpooling ridonkulous taxi lin...   -0.1027  0.055   \n",
       "1458                     oh snap 80 party hosted google    0.5951  0.000   \n",
       "992   brown paper window line grows popup apple stor...    0.0000  0.000   \n",
       "218                            hobo shotgun iphone game    0.0000  0.000   \n",
       "3718  google going circle sn we're launching product...    0.0000  0.000   \n",
       "\n",
       "        neu    pos  punc_emphasis  capt_emphasis  \n",
       "2687  1.000  0.000              0              4  \n",
       "5028  1.000  0.000              0             12  \n",
       "3215  0.882  0.118              0             11  \n",
       "7888  0.792  0.000              5              2  \n",
       "796   1.000  0.000              0             10  \n",
       "...     ...    ...            ...            ...  \n",
       "3739  0.945  0.000              0              3  \n",
       "1458  0.699  0.301              5              8  \n",
       "992   1.000  0.000              0             13  \n",
       "218   1.000  0.000              0             17  \n",
       "3718  1.000  0.000              0             12  \n",
       "\n",
       "[6354 rows x 7 columns]"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'google map app save life regular basis map 150 million user 40 user mobile'"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_2['text'][2687]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-269-e980c4ff8f28>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train_2['text'] = X_train_2['text'].apply(lambda x: x.split())\n",
      "<ipython-input-269-e980c4ff8f28>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train_2['text'] = X_train_2['text'].map(' '.join)\n"
     ]
    }
   ],
   "source": [
    "### Tokenize the training data with simple split of words, flattening to prepare for vectorization\n",
    "X_train_2['text'] = X_train_2['text'].apply(lambda x: x.split())\n",
    "X_train_2['text'] = X_train_2['text'].map(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'google map app save life regular basis map 150 million user 40 user mobile'"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_2['text'][2687]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_model = imbpipeline([\n",
    "    ('cvec', CountVectorizer(encoding = 'iso-8859-1', lowercase = False)),\n",
    "    ('smote', SMOTE(sampling_strategy='minority', random_state=1337)),\n",
    "    ('lr', LogisticRegression(random_state=1337, max_iter=1000))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [7, 6354]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-272-2c927f55bfdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msecond_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/imblearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \"\"\"\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[1;32m    279\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/imblearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    234\u001b[0m                 )\n\u001b[1;32m    235\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit_resample\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                 X, y, fitted_transformer = fit_resample_one_cached(\n\u001b[0m\u001b[1;32m    237\u001b[0m                     \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                     \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Pipeline'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/imblearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_resample_one\u001b[0;34m(sampler, X, y, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    401\u001b[0m                       **fit_params):\n\u001b[1;32m    402\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mX_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0marrays_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArraysTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinarize_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         self.sampling_strategy_ = check_sampling_strategy(\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/imblearn/base.py\u001b[0m in \u001b[0;36m_check_X_y\u001b[0;34m(self, X, y, accept_sparse)\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0maccept_sparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinarize_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_target_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindicate_one_vs_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         X, y = self._validate_data(\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         )\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[1;32m    256\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [7, 6354]"
     ]
    }
   ],
   "source": [
    "second_model.fit(X_train_2, y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Training Accuracy Score:', baseline.score(X_train_2, y_train_2))\n",
    "print('Validation Accuracy Score:', cross_val_score(baseline, X_train_1, y_train_1, cv=5).mean())\n",
    "\n",
    "### Import confusion_matrix and print the confusion matrix for the validation data\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred_1 = baseline.predict(X_test_1)\n",
    "cm = confusion_matrix(y_test_1, y_pred_1)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=baseline.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the and vectorize the training data using TFIDVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "X_train_2 = cv.fit_transform(X_train_2['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6354x7122 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 57972 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000, random_state=1337)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=1337, max_iter=1000)\n",
    "lr.fit(X_train_2, y_train_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy Score: 0.8991186654076172\n"
     ]
    }
   ],
   "source": [
    "print('Training Accuracy Score:', lr.score(X_train_2, y_train_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores:  [0.65066876 0.67820614 0.6884343  0.6711251  0.69291339]\n",
      "Mean Cross Validation Score:  0.6762695379049295\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(lr, X_train_2, y_train_2, cv=5)\n",
    "print('Cross Validation Scores: ', scores)\n",
    "print('Mean Cross Validation Score: ', scores.mean()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.05702658, -0.02378017, -0.01927102, ...,  0.27820585,\n",
       "        -0.06440298, -0.04509425],\n",
       "       [ 0.28577321,  0.17652163,  0.29010016, ...,  0.17998232,\n",
       "        -0.07144259,  0.15929976],\n",
       "       [-0.34279979, -0.15274145, -0.27082915, ..., -0.45818818,\n",
       "         0.13584557, -0.11420551]])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Appears to be overfitting to the training data, but this is expected with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Creating an imbpalance-learn pipeline that uses SMOTE to oversample the minority classes and then uses a Logistic Regression to predict the emotion of a tweet\n",
    "\n",
    "# baseline2 = imbpipeline([\n",
    "#      ('cvec', CountVectorizer(encoding = 'iso-8859-1', lowercase = False)),\n",
    "#      ('smote', SMOTE(sampling_strategy='minority', random_state=1337)),\n",
    "#      ('lr', LogisticRegression(random_state=1337, max_iter=1000))\n",
    "#  ])\n",
    "\n",
    "### Fitting the pipeline to the training data and printing the training and validation accuracy scores\n",
    "\n",
    "# baseline2.fit(X_train_2, y_train_2.values.ravel())\n",
    "# print('Training Accuracy Score:', baseline2.score(X_train_2, y_train_2))\n",
    "# print('Validation Accuracy Score:', cross_val_score(baseline2, X_train_2, y_train_2, cv=5).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-216-3bf9729f6b71>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train_3['text'] = X_train_3['text'].apply(clean_text)\n",
      "<ipython-input-216-3bf9729f6b71>:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train_3['text'] = X_train_3['text'].apply(lambda x: x.split())\n",
      "<ipython-input-216-3bf9729f6b71>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train_3['text'] = X_train_3['text'].map(' '.join)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores:  [0.66955153 0.66719119 0.66404406 0.65460268 0.65984252]\n",
      "Mean Cross Validation Score:  0.6630463953610835\n"
     ]
    }
   ],
   "source": [
    "### Splitting the features into features and the target, including the 'text', sentiment columns, and emphasis columns as features\n",
    "\n",
    "X3 = data[['text', 'compound', 'neg', 'neu', 'pos', 'punc_emphasis','capt_emphasis']]\n",
    "y3 = data['emotion']\n",
    "\n",
    "### Performing a train test split on the data\n",
    "\n",
    "X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(X3, y3, test_size=0.2, random_state=1337)\n",
    "\n",
    "### TRAIN - Applying the clean_text function to the training data\n",
    "\n",
    "X_train_3['text'] = X_train_3['text'].apply(clean_text)\n",
    "\n",
    "### TRAIN - Tokenizing the training data with a simple split of words, and then flattening to prepare for vectorization\n",
    "\n",
    "X_train_3['text'] = X_train_3['text'].apply(lambda x: x.split())\n",
    "X_train_3['text'] = X_train_3['text'].map(' '.join)\n",
    "\n",
    "### TRAIN - Vectorizing the training data using a TD-IDF Vectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train_3 = tfidf.fit_transform(X_train_3['text'])\n",
    "\n",
    "### TRAIN - Fit the training data to a Random Forest Classifier\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train_3, y_train_3)\n",
    "\n",
    "### VALIDATION - Perform a cross validation on the Random Forest Classifier\n",
    "\n",
    "scores = cross_val_score(rfc, X_train_3, y_train_3, cv=5)\n",
    "print('Cross Validation Scores: ', scores)\n",
    "print('Mean Cross Validation Score: ', scores.mean()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Test Data on Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-217-a664f117a575>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_2['text'] = X_test_2['text'].apply(clean_text)\n",
      "<ipython-input-217-a664f117a575>:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_2['text'] = X_test_2['text'].apply(lambda x: x.split())\n",
      "<ipython-input-217-a664f117a575>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_2['text'] = X_test_2['text'].map(' '.join)\n",
      "<ipython-input-217-a664f117a575>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_3['text'] = X_test_3['text'].apply(clean_text)\n",
      "<ipython-input-217-a664f117a575>:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_3['text'] = X_test_3['text'].apply(lambda x: x.split())\n",
      "<ipython-input-217-a664f117a575>:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_3['text'] = X_test_3['text'].map(' '.join)\n"
     ]
    }
   ],
   "source": [
    "### Decision Tree Test Set Preprocessing\n",
    "\n",
    "X_test_1 = X_test_1.apply(clean_text)\n",
    "X_test_1 = X_test_1.apply(lambda x: x.split())\n",
    "X_test_1 = X_test_1.map(' '.join)\n",
    "X_test_1 = cv.transform(X_test_1)\n",
    "\n",
    "### Naive Bayes Test Set Preprocessing\n",
    "\n",
    "X_test_2['text'] = X_test_2['text'].apply(clean_text)\n",
    "X_test_2['text'] = X_test_2['text'].apply(lambda x: x.split())\n",
    "X_test_2['text'] = X_test_2['text'].map(' '.join)\n",
    "X_test_2 = cv.transform(X_test_2['text'])\n",
    "\n",
    "### Random Forest Test Set Preprocessing\n",
    "\n",
    "X_test_3['text'] = X_test_3['text'].apply(clean_text)\n",
    "X_test_3['text'] = X_test_3['text'].apply(lambda x: x.split())\n",
    "X_test_3['text'] = X_test_3['text'].map(' '.join)\n",
    "X_test_3 = tfidf.transform(X_test_3['text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "dba4eb4192a401b10630bbfa25b7fb709bd78a83adcb3ebf371257b880947706"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
